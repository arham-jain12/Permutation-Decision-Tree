{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8DXU9WKCp497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ionosphere\n",
      "\n",
      "Maximum n_estimator and corresponding depth:\n",
      "n_estimator: 50.0\n",
      "Depth: 5\n",
      "\n",
      "Performance metrics on test set:\n",
      "Accuracy_test_rf: 0.9436619718309859\n",
      "F1 Score_test_rf: 0.9393162393162393\n",
      "Precision_test_rf: 0.9574468085106382\n",
      "Recall_test_rf: 0.9285714285714286\n",
      "iris\n",
      "\n",
      "Maximum n_estimator and corresponding depth:\n",
      "n_estimator: 1000.0\n",
      "Depth: 2\n",
      "\n",
      "Performance metrics on test set:\n",
      "Accuracy_test_rf: 1.0\n",
      "F1 Score_test_rf: 1.0\n",
      "Precision_test_rf: 1.0\n",
      "Recall_test_rf: 1.0\n",
      "breastcancerwisconsin\n",
      "\n",
      "Maximum n_estimator and corresponding depth:\n",
      "n_estimator: 50.0\n",
      "Depth: 4\n",
      "\n",
      "Performance metrics on test set:\n",
      "Accuracy_test_rf: 0.9649122807017544\n",
      "F1 Score_test_rf: 0.9623015873015872\n",
      "Precision_test_rf: 0.9672569328433009\n",
      "Recall_test_rf: 0.9580740255486406\n",
      "wine\n",
      "\n",
      "Maximum n_estimator and corresponding depth:\n",
      "n_estimator: 50.0\n",
      "Depth: 3\n",
      "\n",
      "Performance metrics on test set:\n",
      "Accuracy_test_rf: 0.9722222222222222\n",
      "F1 Score_test_rf: 0.9680464778503994\n",
      "Precision_test_rf: 0.9629629629629629\n",
      "Recall_test_rf: 0.9761904761904763\n",
      "diabetespimaindian\n",
      "\n",
      "Maximum n_estimator and corresponding depth:\n",
      "n_estimator: 10.0\n",
      "Depth: 8\n",
      "\n",
      "Performance metrics on test set:\n",
      "Accuracy_test_rf: 0.8506493506493507\n",
      "F1 Score_test_rf: 0.8392557295212162\n",
      "Precision_test_rf: 0.8358477011494253\n",
      "Recall_test_rf: 0.8434343434343434\n",
      "sonar\n",
      "\n",
      "Maximum n_estimator and corresponding depth:\n",
      "n_estimator: 10.0\n",
      "Depth: 6\n",
      "\n",
      "Performance metrics on test set:\n",
      "Accuracy_test_rf: 0.7619047619047619\n",
      "F1 Score_test_rf: 0.7597254004576659\n",
      "Precision_test_rf: 0.7681818181818182\n",
      "Recall_test_rf: 0.7836538461538461\n",
      "appendicitis\n",
      "\n",
      "Maximum n_estimator and corresponding depth:\n",
      "n_estimator: 50.0\n",
      "Depth: 2\n",
      "\n",
      "Performance metrics on test set:\n",
      "Accuracy_test_rf: 0.7727272727272727\n",
      "F1 Score_test_rf: 0.6507936507936507\n",
      "Precision_test_rf: 0.7280701754385965\n",
      "Recall_test_rf: 0.6354166666666666\n",
      "nyse_stock_data.data\n",
      "\n",
      "Maximum n_estimator and corresponding depth:\n",
      "n_estimator: 10.0\n",
      "Depth: 4\n",
      "\n",
      "Performance metrics on test set:\n",
      "Accuracy_test_rf: 0.978328173374613\n",
      "F1 Score_test_rf: 0.9679989248469943\n",
      "Precision_test_rf: 0.9651610305958133\n",
      "Recall_test_rf: 0.9709899336615585\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import numpy as np\n",
    "from load import load\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "# Define the path to the datasets folder\n",
    "datasets_folder = os.path.join(current_directory, \"datasets\")\n",
    "# datasets = dataset for dataset in os.listdir(datasets_folder) if not (dataset.startswith('.')\n",
    "datasets = [dataset for dataset in os.listdir(datasets_folder) if not (dataset.startswith('.'))]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    # importing dataset\n",
    "    print(dataset_name)\n",
    "    X, Y = load(dataset_name)\n",
    "        # Define the number of splits for time series split\n",
    "    num_splits = 5\n",
    "\n",
    "    # Fix the seed value for splitting data\n",
    "    split_seed = 42\n",
    "\n",
    "    # Define the range of depths for the decision tree\n",
    "    depth_range = range(2, 21)\n",
    "\n",
    "    # Initialize a list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Define the range of estimators for the random forest\n",
    "    estimators_range = [10, 50, 100, 150, 1000]\n",
    "\n",
    "    # declaring test size for time series split\n",
    "    time_series_split_test_size = 15\n",
    "\n",
    "    # Perform time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=num_splits, test_size=time_series_split_test_size)\n",
    "\n",
    "    # _______________________________________________________________________________\n",
    "\n",
    "    # Variable description:\n",
    "    # _______________________________________________________________________________\n",
    "\n",
    "    #     X               -   Data attributes.\n",
    "    #     y               -   Corresponding labels for X.\n",
    "    #     X_train         -   Data attributes for training (80% of the dataset).\n",
    "    #     y_train         -   Corresponding labels for X_train.\n",
    "    #     X_test          -   Data attributes for testing (20% of the dataset).\n",
    "    #     y_test          -   Corresponding labels for X_test.\n",
    "\n",
    "\n",
    "    # Split the data into training and testing sets with the fixed seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=split_seed)\n",
    "\n",
    "    # Iterate over different numbers of estimators for Random Forest\n",
    "    for n_estimators in estimators_range:\n",
    "        # Iterate over different depths for the decision tree\n",
    "        for depth in depth_range:\n",
    "            # Initialize lists to store evaluation metrics for each fold\n",
    "            accuracies = []\n",
    "            f1_scores = []\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "\n",
    "            # Perform time series split\n",
    "            for train_index, test_index in tscv.split(X_train):\n",
    "                X_cv_train, X_cv_test = pd.DataFrame(X_train).iloc[train_index], pd.DataFrame(X_train).iloc[test_index]\n",
    "                y_cv_train, y_cv_test = pd.DataFrame(y_train).iloc[train_index], pd.DataFrame(y_train).iloc[test_index]\n",
    "\n",
    "                # Train the Random Forest model with the current number of estimators and depth\n",
    "                rf_classifier = RandomForestClassifier(criterion='entropy', n_estimators=n_estimators, max_depth=depth, random_state= 21)\n",
    "                y_cv_train = np.ravel(y_cv_train)\n",
    "                rf_classifier.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "                # Make predictions\n",
    "                y_pred = rf_classifier.predict(X_cv_test)\n",
    "\n",
    "                # Calculate evaluation metrics for this fold\n",
    "                accuracy = accuracy_score(y_cv_test, y_pred)\n",
    "                f1 = f1_score(y_cv_test, y_pred, average='macro')  # Use macro F1 score\n",
    "                precision = precision_score(y_cv_test, y_pred, average='macro', zero_division=1)\n",
    "                recall = recall_score(y_cv_test, y_pred, average='macro', zero_division=1)\n",
    "\n",
    "                # Append metrics to the lists\n",
    "                accuracies.append(accuracy)\n",
    "                f1_scores.append(f1)\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "\n",
    "            # Calculate mean metrics across all folds\n",
    "            mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "            mean_f1 = sum(f1_scores) / len(f1_scores)\n",
    "            mean_precision = sum(precisions) / len(precisions)\n",
    "            mean_recall = sum(recalls) / len(recalls)\n",
    "\n",
    "            # Store the results for this seed value and depth\n",
    "            results.append({\n",
    "                'Split Seed': split_seed,\n",
    "                'N_estimators': n_estimators,\n",
    "                'Depth of Tree': depth,\n",
    "                'Mean Accuracy': mean_accuracy,\n",
    "                'Mean F1 Score': mean_f1,\n",
    "                'Mean Precision': mean_precision,\n",
    "                'Mean Recall': mean_recall\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Group by 'N_estimators' and find the row with maximum 'Mean F1 Score' for each group\n",
    "    max_f1_per_estimator = results_df.loc[results_df.groupby('N_estimators')['Mean F1 Score'].idxmax()]\n",
    "\n",
    "    # Find the row with the maximum 'Mean F1 Score' across all 'N_estimators'\n",
    "    max_f1_row = max_f1_per_estimator.loc[max_f1_per_estimator['Mean F1 Score'].idxmax()]\n",
    "\n",
    "    # Extract the maximum n_estimator and corresponding depth with the minimum value\n",
    "    max_n_estimator = max_f1_row['N_estimators']\n",
    "    max_depth_for_estimator = max_f1_per_estimator.loc[max_f1_per_estimator['N_estimators'] == max_n_estimator, 'Depth of Tree'].min()\n",
    "\n",
    "    print(\"\\nMaximum n_estimator and corresponding depth:\")\n",
    "    print(\"n_estimator:\", max_n_estimator)\n",
    "    print(\"Depth:\", max_depth_for_estimator) \n",
    "\n",
    "        # Train Random Forest classifier with the best parameters\n",
    "    best_rf_classifier = RandomForestClassifier(criterion='entropy',n_estimators=int(max_n_estimator), max_depth=max_depth_for_estimator, random_state= 21)\n",
    "    y_train = np.ravel(y_train)\n",
    "    best_rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_test_rf = best_rf_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_test_rf = accuracy_score(y_test, y_pred_test_rf)\n",
    "    precision_test_rf = precision_score(y_test, y_pred_test_rf, average='macro', zero_division=1)\n",
    "    recall_test_rf = recall_score(y_test, y_pred_test_rf, average='macro', zero_division=1)\n",
    "    f1_test_rf = f1_score(y_test, y_pred_test_rf, average='macro')\n",
    "\n",
    "    print(\"\\nPerformance metrics on test set:\")\n",
    "    print(\"Accuracy_test_rf:\", accuracy_test_rf)\n",
    "    print(\"F1 Score_test_rf:\", f1_test_rf)\n",
    "    print(\"Precision_test_rf:\", precision_test_rf)\n",
    "    print(\"Recall_test_rf:\", recall_test_rf)\n",
    "\n",
    "    # Creating this because we want to save the result in form of csv and numpy\n",
    "    evaluation_metrics_rf_test = {\n",
    "        \"Accuracy\": accuracy_test_rf,\n",
    "        \"F1 Score\": f1_test_rf,\n",
    "        \"Precision\": precision_test_rf,\n",
    "        \"Recall\": recall_test_rf,\n",
    "        \"best_N_estimator_\":max_n_estimator,\n",
    "        \"Corresponding_depth\" : max_depth_for_estimator\n",
    "    }\n",
    "\n",
    "    # Define the path to the results folder\n",
    "    results_folder = os.path.join(current_directory, \"results\")\n",
    "\n",
    "    # Define the dataset name\n",
    "    dataset_name = dataset_name\n",
    "\n",
    "    # Create a folder for the current dataset within the results directory\n",
    "    dataset_results_folder = os.path.join(results_folder, dataset_name)\n",
    "    os.makedirs(dataset_results_folder, exist_ok=True)\n",
    "\n",
    "    # Create folder for random forest results\n",
    "    rf_folder = os.path.join(dataset_results_folder, \"RF_entropy\")\n",
    "    os.makedirs(rf_folder, exist_ok=True)\n",
    "\n",
    "    # Save results_df to CSV\n",
    "    results_csv_path = os.path.join(rf_folder, \"all_nestimator_with_depth.csv\")\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    # Save results_df to NumPy\n",
    "    results_npy_path = os.path.join(rf_folder, \"all_nestimator_with_depth.npy\")\n",
    "    np.save(results_npy_path, results)\n",
    "\n",
    "    # Path for DT csv file\n",
    "    rf_metrics_csv_path = os.path.join(rf_folder, \"evaluation_metrics_rf_test.csv\")\n",
    "    # saving this to DT folder\n",
    "    pd.DataFrame(evaluation_metrics_rf_test.items(), columns=[\"Metric\", \"Value\"]).to_csv(rf_metrics_csv_path, index=False)\n",
    "\n",
    "    # Path for RF numpy file\n",
    "    rf_metrics_npy_path = os.path.join(rf_folder, \"evaluation_metrics_rf_test.npy\")\n",
    "    # saving it to DT folder\n",
    "    np.save(rf_metrics_npy_path, evaluation_metrics_rf_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
